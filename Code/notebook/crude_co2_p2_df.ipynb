{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>crude_oil</th>\n",
       "      <th>cpi</th>\n",
       "      <th>coal_consumption</th>\n",
       "      <th>ngas_consumption</th>\n",
       "      <th>us_pop</th>\n",
       "      <th>oil_production</th>\n",
       "      <th>cattle_index</th>\n",
       "      <th>tree_cover_loss</th>\n",
       "      <th>us_real_gdp</th>\n",
       "      <th>usd_mex_exrate</th>\n",
       "      <th>co2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>42.40</td>\n",
       "      <td>93.262178</td>\n",
       "      <td>80527.66971</td>\n",
       "      <td>1.166511e+06</td>\n",
       "      <td>306035.0</td>\n",
       "      <td>4935</td>\n",
       "      <td>297.27</td>\n",
       "      <td>1406957.794</td>\n",
       "      <td>14476.2498</td>\n",
       "      <td>13.657700</td>\n",
       "      <td>386.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-11</td>\n",
       "      <td>44.46</td>\n",
       "      <td>91.817169</td>\n",
       "      <td>81500.87953</td>\n",
       "      <td>9.357362e+05</td>\n",
       "      <td>306085.5</td>\n",
       "      <td>4917</td>\n",
       "      <td>288.70</td>\n",
       "      <td>1567991.001</td>\n",
       "      <td>16075.1555</td>\n",
       "      <td>13.899500</td>\n",
       "      <td>387.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-18</td>\n",
       "      <td>36.73</td>\n",
       "      <td>91.122475</td>\n",
       "      <td>97916.98177</td>\n",
       "      <td>9.389947e+05</td>\n",
       "      <td>306136.0</td>\n",
       "      <td>5052</td>\n",
       "      <td>299.23</td>\n",
       "      <td>1336297.836</td>\n",
       "      <td>15137.1916</td>\n",
       "      <td>14.025100</td>\n",
       "      <td>387.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-25</td>\n",
       "      <td>42.15</td>\n",
       "      <td>87.888142</td>\n",
       "      <td>84498.02661</td>\n",
       "      <td>9.046054e+05</td>\n",
       "      <td>306186.5</td>\n",
       "      <td>5045</td>\n",
       "      <td>292.83</td>\n",
       "      <td>1652557.380</td>\n",
       "      <td>15155.3440</td>\n",
       "      <td>14.255700</td>\n",
       "      <td>387.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-02-01</td>\n",
       "      <td>42.70</td>\n",
       "      <td>82.715265</td>\n",
       "      <td>89510.77745</td>\n",
       "      <td>7.992948e+05</td>\n",
       "      <td>306237.0</td>\n",
       "      <td>5235</td>\n",
       "      <td>290.44</td>\n",
       "      <td>1470782.481</td>\n",
       "      <td>15538.1713</td>\n",
       "      <td>14.119200</td>\n",
       "      <td>387.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>58.07</td>\n",
       "      <td>112.084710</td>\n",
       "      <td>43325.23094</td>\n",
       "      <td>6.060118e+05</td>\n",
       "      <td>329314.0</td>\n",
       "      <td>12900</td>\n",
       "      <td>430.72</td>\n",
       "      <td>2173871.578</td>\n",
       "      <td>19477.3091</td>\n",
       "      <td>19.286699</td>\n",
       "      <td>411.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2019-12-08</td>\n",
       "      <td>57.64</td>\n",
       "      <td>109.471205</td>\n",
       "      <td>39751.66796</td>\n",
       "      <td>8.286665e+05</td>\n",
       "      <td>329335.8</td>\n",
       "      <td>12800</td>\n",
       "      <td>426.54</td>\n",
       "      <td>2458219.683</td>\n",
       "      <td>19750.1111</td>\n",
       "      <td>18.997499</td>\n",
       "      <td>411.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>59.25</td>\n",
       "      <td>108.183411</td>\n",
       "      <td>48847.79099</td>\n",
       "      <td>7.519214e+05</td>\n",
       "      <td>329357.6</td>\n",
       "      <td>12800</td>\n",
       "      <td>435.32</td>\n",
       "      <td>1946025.679</td>\n",
       "      <td>20106.3904</td>\n",
       "      <td>18.938499</td>\n",
       "      <td>411.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>2019-12-22</td>\n",
       "      <td>60.75</td>\n",
       "      <td>108.707988</td>\n",
       "      <td>50008.88542</td>\n",
       "      <td>5.915937e+05</td>\n",
       "      <td>329379.4</td>\n",
       "      <td>12900</td>\n",
       "      <td>429.35</td>\n",
       "      <td>2230631.393</td>\n",
       "      <td>20396.2701</td>\n",
       "      <td>18.844700</td>\n",
       "      <td>412.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>61.29</td>\n",
       "      <td>105.725844</td>\n",
       "      <td>49428.07791</td>\n",
       "      <td>6.146294e+05</td>\n",
       "      <td>329401.2</td>\n",
       "      <td>12900</td>\n",
       "      <td>432.42</td>\n",
       "      <td>2124357.069</td>\n",
       "      <td>18812.8854</td>\n",
       "      <td>18.911301</td>\n",
       "      <td>412.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           week  crude_oil         cpi  coal_consumption  ngas_consumption  \\\n",
       "0    2009-01-04      42.40   93.262178       80527.66971      1.166511e+06   \n",
       "1    2009-01-11      44.46   91.817169       81500.87953      9.357362e+05   \n",
       "2    2009-01-18      36.73   91.122475       97916.98177      9.389947e+05   \n",
       "3    2009-01-25      42.15   87.888142       84498.02661      9.046054e+05   \n",
       "4    2009-02-01      42.70   82.715265       89510.77745      7.992948e+05   \n",
       "..          ...        ...         ...               ...               ...   \n",
       "569  2019-12-01      58.07  112.084710       43325.23094      6.060118e+05   \n",
       "570  2019-12-08      57.64  109.471205       39751.66796      8.286665e+05   \n",
       "571  2019-12-15      59.25  108.183411       48847.79099      7.519214e+05   \n",
       "572  2019-12-22      60.75  108.707988       50008.88542      5.915937e+05   \n",
       "573  2019-12-29      61.29  105.725844       49428.07791      6.146294e+05   \n",
       "\n",
       "       us_pop  oil_production  cattle_index  tree_cover_loss  us_real_gdp  \\\n",
       "0    306035.0            4935        297.27      1406957.794   14476.2498   \n",
       "1    306085.5            4917        288.70      1567991.001   16075.1555   \n",
       "2    306136.0            5052        299.23      1336297.836   15137.1916   \n",
       "3    306186.5            5045        292.83      1652557.380   15155.3440   \n",
       "4    306237.0            5235        290.44      1470782.481   15538.1713   \n",
       "..        ...             ...           ...              ...          ...   \n",
       "569  329314.0           12900        430.72      2173871.578   19477.3091   \n",
       "570  329335.8           12800        426.54      2458219.683   19750.1111   \n",
       "571  329357.6           12800        435.32      1946025.679   20106.3904   \n",
       "572  329379.4           12900        429.35      2230631.393   20396.2701   \n",
       "573  329401.2           12900        432.42      2124357.069   18812.8854   \n",
       "\n",
       "     usd_mex_exrate     co2  \n",
       "0         13.657700  386.40  \n",
       "1         13.899500  387.02  \n",
       "2         14.025100  387.14  \n",
       "3         14.255700  387.50  \n",
       "4         14.119200  387.03  \n",
       "..              ...     ...  \n",
       "569       19.286699  411.07  \n",
       "570       18.997499  411.32  \n",
       "571       18.938499  411.89  \n",
       "572       18.844700  412.21  \n",
       "573       18.911301  412.99  \n",
       "\n",
       "[574 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data TODO: Change the file path to the dataset this question needed\n",
    "df = pd.read_csv('..\\\\source_data\\\\crude_co2_p2_df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating independent and dependent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>crude_oil</th>\n",
       "      <th>cpi</th>\n",
       "      <th>coal_consumption</th>\n",
       "      <th>ngas_consumption</th>\n",
       "      <th>us_pop</th>\n",
       "      <th>oil_production</th>\n",
       "      <th>cattle_index</th>\n",
       "      <th>tree_cover_loss</th>\n",
       "      <th>us_real_gdp</th>\n",
       "      <th>usd_mex_exrate</th>\n",
       "      <th>co2</th>\n",
       "      <th>of_chg_1</th>\n",
       "      <th>of_chg_2</th>\n",
       "      <th>of_chg_3</th>\n",
       "      <th>of_chg_5</th>\n",
       "      <th>of_chg_10</th>\n",
       "      <th>co2_chg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009-03-22</td>\n",
       "      <td>49.49</td>\n",
       "      <td>93.593485</td>\n",
       "      <td>88029.33328</td>\n",
       "      <td>607862.7477</td>\n",
       "      <td>306562.2</td>\n",
       "      <td>5432</td>\n",
       "      <td>282.76</td>\n",
       "      <td>1933223.874</td>\n",
       "      <td>15121.9490</td>\n",
       "      <td>14.338800</td>\n",
       "      <td>389.29</td>\n",
       "      <td>0.083881</td>\n",
       "      <td>0.146132</td>\n",
       "      <td>0.204136</td>\n",
       "      <td>0.339740</td>\n",
       "      <td>0.113135</td>\n",
       "      <td>0.002808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009-03-29</td>\n",
       "      <td>52.99</td>\n",
       "      <td>105.508484</td>\n",
       "      <td>81588.96621</td>\n",
       "      <td>786643.0421</td>\n",
       "      <td>306603.6</td>\n",
       "      <td>5480</td>\n",
       "      <td>278.41</td>\n",
       "      <td>1451331.110</td>\n",
       "      <td>15312.9473</td>\n",
       "      <td>13.541800</td>\n",
       "      <td>389.04</td>\n",
       "      <td>0.070721</td>\n",
       "      <td>0.160534</td>\n",
       "      <td>0.227189</td>\n",
       "      <td>0.426380</td>\n",
       "      <td>0.442690</td>\n",
       "      <td>-0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009-04-05</td>\n",
       "      <td>50.34</td>\n",
       "      <td>97.283677</td>\n",
       "      <td>72928.73097</td>\n",
       "      <td>389615.9568</td>\n",
       "      <td>306645.0</td>\n",
       "      <td>5469</td>\n",
       "      <td>287.29</td>\n",
       "      <td>1181283.104</td>\n",
       "      <td>15389.4132</td>\n",
       "      <td>13.130000</td>\n",
       "      <td>389.43</td>\n",
       "      <td>-0.050009</td>\n",
       "      <td>0.017175</td>\n",
       "      <td>0.102497</td>\n",
       "      <td>0.224818</td>\n",
       "      <td>0.194306</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009-04-12</td>\n",
       "      <td>50.46</td>\n",
       "      <td>79.216384</td>\n",
       "      <td>72626.84388</td>\n",
       "      <td>553381.6950</td>\n",
       "      <td>306699.5</td>\n",
       "      <td>5482</td>\n",
       "      <td>288.74</td>\n",
       "      <td>1278670.711</td>\n",
       "      <td>14744.1587</td>\n",
       "      <td>13.097800</td>\n",
       "      <td>388.93</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.047745</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.168597</td>\n",
       "      <td>0.181733</td>\n",
       "      <td>-0.001284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009-04-19</td>\n",
       "      <td>49.86</td>\n",
       "      <td>95.223458</td>\n",
       "      <td>69962.81952</td>\n",
       "      <td>351902.9953</td>\n",
       "      <td>306754.0</td>\n",
       "      <td>5421</td>\n",
       "      <td>288.48</td>\n",
       "      <td>1752394.499</td>\n",
       "      <td>14624.3705</td>\n",
       "      <td>13.550500</td>\n",
       "      <td>389.56</td>\n",
       "      <td>-0.011891</td>\n",
       "      <td>-0.009535</td>\n",
       "      <td>-0.059068</td>\n",
       "      <td>0.091984</td>\n",
       "      <td>0.222658</td>\n",
       "      <td>0.001620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>58.07</td>\n",
       "      <td>112.084710</td>\n",
       "      <td>43325.23094</td>\n",
       "      <td>606011.7971</td>\n",
       "      <td>329314.0</td>\n",
       "      <td>12900</td>\n",
       "      <td>430.72</td>\n",
       "      <td>2173871.578</td>\n",
       "      <td>19477.3091</td>\n",
       "      <td>19.286699</td>\n",
       "      <td>411.07</td>\n",
       "      <td>0.020562</td>\n",
       "      <td>0.021460</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.051993</td>\n",
       "      <td>-0.021237</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2019-12-08</td>\n",
       "      <td>57.64</td>\n",
       "      <td>109.471205</td>\n",
       "      <td>39751.66796</td>\n",
       "      <td>828666.4868</td>\n",
       "      <td>329335.8</td>\n",
       "      <td>12800</td>\n",
       "      <td>426.54</td>\n",
       "      <td>2458219.683</td>\n",
       "      <td>19750.1111</td>\n",
       "      <td>18.997499</td>\n",
       "      <td>411.32</td>\n",
       "      <td>-0.007405</td>\n",
       "      <td>0.013005</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.044771</td>\n",
       "      <td>0.013005</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>59.25</td>\n",
       "      <td>108.183411</td>\n",
       "      <td>48847.79099</td>\n",
       "      <td>751921.3538</td>\n",
       "      <td>329357.6</td>\n",
       "      <td>12800</td>\n",
       "      <td>435.32</td>\n",
       "      <td>1946025.679</td>\n",
       "      <td>20106.3904</td>\n",
       "      <td>18.938499</td>\n",
       "      <td>411.89</td>\n",
       "      <td>0.027932</td>\n",
       "      <td>0.020320</td>\n",
       "      <td>0.041301</td>\n",
       "      <td>0.045158</td>\n",
       "      <td>0.115399</td>\n",
       "      <td>0.001386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>2019-12-22</td>\n",
       "      <td>60.75</td>\n",
       "      <td>108.707988</td>\n",
       "      <td>50008.88542</td>\n",
       "      <td>591593.7371</td>\n",
       "      <td>329379.4</td>\n",
       "      <td>12900</td>\n",
       "      <td>429.35</td>\n",
       "      <td>2230631.393</td>\n",
       "      <td>20396.2701</td>\n",
       "      <td>18.844700</td>\n",
       "      <td>412.21</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.053956</td>\n",
       "      <td>0.046151</td>\n",
       "      <td>0.068602</td>\n",
       "      <td>0.140417</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>61.29</td>\n",
       "      <td>105.725844</td>\n",
       "      <td>49428.07791</td>\n",
       "      <td>614629.4116</td>\n",
       "      <td>329401.2</td>\n",
       "      <td>12900</td>\n",
       "      <td>432.42</td>\n",
       "      <td>2124357.069</td>\n",
       "      <td>18812.8854</td>\n",
       "      <td>18.911301</td>\n",
       "      <td>412.99</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.034430</td>\n",
       "      <td>0.063324</td>\n",
       "      <td>0.077153</td>\n",
       "      <td>0.145822</td>\n",
       "      <td>0.001892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>563 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           week  crude_oil         cpi  coal_consumption  ngas_consumption  \\\n",
       "11   2009-03-22      49.49   93.593485       88029.33328       607862.7477   \n",
       "12   2009-03-29      52.99  105.508484       81588.96621       786643.0421   \n",
       "13   2009-04-05      50.34   97.283677       72928.73097       389615.9568   \n",
       "14   2009-04-12      50.46   79.216384       72626.84388       553381.6950   \n",
       "15   2009-04-19      49.86   95.223458       69962.81952       351902.9953   \n",
       "..          ...        ...         ...               ...               ...   \n",
       "569  2019-12-01      58.07  112.084710       43325.23094       606011.7971   \n",
       "570  2019-12-08      57.64  109.471205       39751.66796       828666.4868   \n",
       "571  2019-12-15      59.25  108.183411       48847.79099       751921.3538   \n",
       "572  2019-12-22      60.75  108.707988       50008.88542       591593.7371   \n",
       "573  2019-12-29      61.29  105.725844       49428.07791       614629.4116   \n",
       "\n",
       "       us_pop  oil_production  cattle_index  tree_cover_loss  us_real_gdp  \\\n",
       "11   306562.2            5432        282.76      1933223.874   15121.9490   \n",
       "12   306603.6            5480        278.41      1451331.110   15312.9473   \n",
       "13   306645.0            5469        287.29      1181283.104   15389.4132   \n",
       "14   306699.5            5482        288.74      1278670.711   14744.1587   \n",
       "15   306754.0            5421        288.48      1752394.499   14624.3705   \n",
       "..        ...             ...           ...              ...          ...   \n",
       "569  329314.0           12900        430.72      2173871.578   19477.3091   \n",
       "570  329335.8           12800        426.54      2458219.683   19750.1111   \n",
       "571  329357.6           12800        435.32      1946025.679   20106.3904   \n",
       "572  329379.4           12900        429.35      2230631.393   20396.2701   \n",
       "573  329401.2           12900        432.42      2124357.069   18812.8854   \n",
       "\n",
       "     usd_mex_exrate     co2  of_chg_1  of_chg_2  of_chg_3  of_chg_5  \\\n",
       "11        14.338800  389.29  0.083881  0.146132  0.204136  0.339740   \n",
       "12        13.541800  389.04  0.070721  0.160534  0.227189  0.426380   \n",
       "13        13.130000  389.43 -0.050009  0.017175  0.102497  0.224818   \n",
       "14        13.097800  388.93  0.002384 -0.047745  0.019600  0.168597   \n",
       "15        13.550500  389.56 -0.011891 -0.009535 -0.059068  0.091984   \n",
       "..              ...     ...       ...       ...       ...       ...   \n",
       "569       19.286699  411.07  0.020562  0.021460  0.024343  0.051993   \n",
       "570       18.997499  411.32 -0.007405  0.013005  0.013896  0.044771   \n",
       "571       18.938499  411.89  0.027932  0.020320  0.041301  0.045158   \n",
       "572       18.844700  412.21  0.025316  0.053956  0.046151  0.068602   \n",
       "573       18.911301  412.99  0.008889  0.034430  0.063324  0.077153   \n",
       "\n",
       "     of_chg_10   co2_chg  \n",
       "11    0.113135  0.002808  \n",
       "12    0.442690 -0.000642  \n",
       "13    0.194306  0.001002  \n",
       "14    0.181733 -0.001284  \n",
       "15    0.222658  0.001620  \n",
       "..         ...       ...  \n",
       "569  -0.021237  0.000852  \n",
       "570   0.013005  0.000608  \n",
       "571   0.115399  0.001386  \n",
       "572   0.140417  0.000777  \n",
       "573   0.145822  0.001892  \n",
       "\n",
       "[563 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate independent variables\n",
    "periods = (1,2,3,5,10)\n",
    "for i in periods:\n",
    "    df['of_chg_{}'.format(i)] = df['crude_oil'].pct_change(periods = i)\n",
    "    \n",
    "# calculate dependent variable\n",
    "df['co2_chg'] = df['co2'].pct_change(periods=1)\n",
    "\n",
    "# eliminate the empty rows\n",
    "df = df[11:]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperating training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.94900000e+01,  8.38808585e-02,  1.46132469e-01, ...,\n",
       "         5.43200000e+03,  1.93322387e+06,  1.51219490e+04],\n",
       "       [ 5.29900000e+01,  7.07213579e-02,  1.60534385e-01, ...,\n",
       "         5.48000000e+03,  1.45133111e+06,  1.53129473e+04],\n",
       "       [ 5.03400000e+01, -5.00094357e-02,  1.71751869e-02, ...,\n",
       "         5.46900000e+03,  1.18128310e+06,  1.53894132e+04],\n",
       "       ...,\n",
       "       [ 5.92500000e+01,  2.79319917e-02,  2.03203031e-02, ...,\n",
       "         1.28000000e+04,  1.94602568e+06,  2.01063904e+04],\n",
       "       [ 6.07500000e+01,  2.53164557e-02,  5.39555864e-02, ...,\n",
       "         1.29000000e+04,  2.23063139e+06,  2.03962701e+04],\n",
       "       [ 6.12900000e+01,  8.88888889e-03,  3.44303797e-02, ...,\n",
       "         1.29000000e+04,  2.12435707e+06,  1.88128854e+04]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of independent variable's name TODO: add the appropriate variable names to the list\n",
    "#ind_var_name = ['of_chg_{}'.format(i) for i in periods]+['cpi','coal_consumption','ngas_consumption','us_pop','oil_production','cattle_index','tree_cover_loss','us_real_gdp','usd_mex_exrate']\n",
    "ind_var_name = ['of_chg_{}'.format(i) for i in periods]+['cpi','coal_consumption','ngas_consumption',\n",
    "                                                         'us_pop','oil_production','tree_cover_loss','us_real_gdp']\n",
    "num_var =len(ind_var_name) # put number of variables here\n",
    "\n",
    "# extract the values to X\n",
    "X = df[['crude_oil'] + ind_var_name].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((563, 13), (563,), (563,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create y for regressor, y_c for classifier\n",
    "y = df['co2_chg']\n",
    "y_c = (df['co2_chg'] > 0).values.astype('int')\n",
    "\n",
    "# set training, validation, and test criterias\n",
    "training_ratio = 0.7\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# check correctness of X and y\n",
    "X.shape, y.shape, y_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:\t (393, 13)\n",
      "X_test shape:\t (85, 13)\n",
      "X_val shape:\t (85, 13)\n",
      "y_train shape:\t (393,)\n",
      "y_val shape:\t (85,)\n",
      "y_test shape:\t (85,)\n"
     ]
    }
   ],
   "source": [
    "# create training, validation, and test sets for regressor\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_ratio, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = validation_ratio/(validation_ratio + training_ratio), random_state = 42)\n",
    "print(\"X_train shape:\\t\", X_train.shape)\n",
    "print(\"X_test shape:\\t\", X_test.shape)\n",
    "print(\"X_val shape:\\t\", X_val.shape)\n",
    "print(\"y_train shape:\\t\", y_train.shape)\n",
    "print(\"y_val shape:\\t\", y_val.shape)\n",
    "print(\"y_test shape:\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_c shape: (393, 13)\n",
      "X_test_c shape:\t (85, 13)\n",
      "X_val_c shape:\t (85, 13)\n",
      "y_train_c shape: (393,)\n",
      "y_val_c shape:\t (85,)\n",
      "y_test_c shape:\t (85,)\n"
     ]
    }
   ],
   "source": [
    "# create training, validation, and test sets for classifier\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X, y_c, test_size = test_ratio, random_state = 42)\n",
    "X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(X_train_c, y_train_c, test_size = validation_ratio/(validation_ratio + training_ratio), random_state = 42)\n",
    "print(\"X_train_c shape:\", X_train_c.shape)\n",
    "print(\"X_test_c shape:\\t\", X_test_c.shape)\n",
    "print(\"X_val_c shape:\\t\", X_val_c.shape)\n",
    "print(\"y_train_c shape:\", y_train_c.shape)\n",
    "print(\"y_val_c shape:\\t\", y_val_c.shape)\n",
    "print(\"y_test_c shape:\\t\", y_test_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:\t (393, 13)\n",
      "X_val shape:\t (85, 13)\n"
     ]
    }
   ],
   "source": [
    "# normalization for regressor set\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train_mean = X_train[:, i]. mean()\n",
    "    X_train_std = X_train[:, i].std()\n",
    "    X_train[:, i] = (X_train[:,i] - X_train_mean) / X_train_std\n",
    "    X_test[:,i] = (X_test[:,i] - X_train_mean) / X_train_std\n",
    "    X_val[:, i] = (X_val[:,i] - X_train_mean) / X_train_std\n",
    "\n",
    "print(\"X_train shape:\\t\", X_train.shape)\n",
    "print(\"X_val shape:\\t\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_c shape: (393, 13)\n",
      "X_val_c shape:\t (85, 13)\n"
     ]
    }
   ],
   "source": [
    "# normalization for classifier set\n",
    "for i in range(X_train_c.shape[1]):\n",
    "    X_train_c_mean = X_train_c[:, i]. mean()\n",
    "    X_train_c_std = X_train_c[:, i].std()\n",
    "    X_train_c[:, i] = (X_train_c[:,i] - X_train_c_mean) / X_train_c_std\n",
    "    X_test_c[:,i] = (X_test_c[:,i] - X_train_c_mean) / X_train_c_std\n",
    "    X_val_c[:, i] = (X_val_c[:,i] - X_train_c_mean) / X_train_c_std\n",
    "\n",
    "print(\"X_train_c shape:\", X_train_c.shape)\n",
    "print(\"X_val_c shape:\\t\", X_val_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca shape:\t (393, 9)\n",
      "X_test_pca shape:\t (85, 9)\n",
      "X_val_pca shape:\t (85, 9)\n",
      "y_train shape:\t (393,)\n",
      "y_val shape:\t (85,)\n",
      "y_test shape:\t (85,)\n"
     ]
    }
   ],
   "source": [
    "# PCA for regressor\n",
    "# fit on training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# apply transform to training and test set\n",
    "X_train_pca = scaler.transform(X_train)\n",
    "X_test_pca = scaler.transform(X_test)\n",
    "X_val_pca = scaler.transform(X_val)\n",
    "\n",
    "# set PCA value\n",
    "pca = PCA(0.95)\n",
    "\n",
    "# fit PCA training set\n",
    "pca.fit(X_train_pca)\n",
    "\n",
    "# apply transform to training and test set\n",
    "X_train_pca = pca.transform(X_train_pca)\n",
    "X_test_pca = pca.transform(X_test_pca)\n",
    "X_val_pca = pca.transform(X_val_pca)\n",
    "print(\"X_train_pca shape:\\t\", X_train_pca.shape)\n",
    "print(\"X_test_pca shape:\\t\", X_test_pca.shape)\n",
    "print(\"X_val_pca shape:\\t\", X_val_pca.shape)\n",
    "print(\"y_train shape:\\t\", y_train.shape)\n",
    "print(\"y_val shape:\\t\", y_val.shape)\n",
    "print(\"y_test shape:\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_c_pca shape:\t (393, 9)\n",
      "X_test_c_pca shape:\t (85, 9)\n",
      "X_val_c_pca shape:\t (85, 9)\n",
      "y_train_c shape: (393,)\n",
      "y_val_c shape:\t (85,)\n",
      "y_test_c shape:\t (85,)\n"
     ]
    }
   ],
   "source": [
    "# PCA for classifier\n",
    "# fit on training set_c\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_c)\n",
    "\n",
    "# apply transform to training and test set\n",
    "X_train_c_pca = scaler.transform(X_train_c)\n",
    "X_test_c_pca = scaler.transform(X_test_c)\n",
    "X_val_c_pca = scaler.transform(X_val_c)\n",
    "\n",
    "# set PCA value\n",
    "pca = PCA(0.95)\n",
    "\n",
    "# fit PCA training set\n",
    "pca.fit(X_train_c_pca)\n",
    "\n",
    "# apply transform to training and test set\n",
    "X_train_c_pca = pca.transform(X_train_c_pca)\n",
    "X_test_c_pca = pca.transform(X_test_c_pca)\n",
    "X_val_c_pca = pca.transform(X_val_c_pca)\n",
    "print(\"X_train_c_pca shape:\\t\", X_train_c_pca.shape)\n",
    "print(\"X_test_c_pca shape:\\t\", X_test_c_pca.shape)\n",
    "print(\"X_val_c_pca shape:\\t\", X_val_c_pca.shape)\n",
    "print(\"y_train_c shape:\", y_train_c.shape)\n",
    "print(\"y_val_c shape:\\t\", y_val_c.shape)\n",
    "print(\"y_test_c shape:\\t\", y_test_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_var = X_train_c_pca.shape[1]\n",
    "num_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_multiple_reg: 0.0733\n"
     ]
    }
   ],
   "source": [
    "multiple_reg = LinearRegression().fit(X_train_pca, y_train)\n",
    "score_multiple_reg = multiple_reg.score(X_test_pca, y_test)\n",
    "print('score_multiple_reg:', \"%.4f\"%score_multiple_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07332280917435552"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to prove this is the correct polynomial regression, set degree to 1\n",
    "poly = PolynomialFeatures(1)\n",
    "# polynomial transform\n",
    "X_train_poly = poly.fit_transform(X_train_pca, y_train)\n",
    "X_val_poly = poly.fit_transform(X_val_pca,y_val)\n",
    "X_test_poly = poly.fit_transform(X_test_pca,y_test)\n",
    "\n",
    "# fit to linear regression\n",
    "polyreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "\n",
    "# predict using liear regression\n",
    "score_poly_reg = polyreg.score(X_test_poly, y_test)\n",
    "score_poly_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_poly_reg, degree 2 : 0.1256\n",
      "score_poly_reg, degree 3 : -6.8749\n",
      "score_poly_reg, degree 4 : -22.2008\n",
      "score_poly_reg, degree 5 : -37.5740\n",
      "max degree: 2\n"
     ]
    }
   ],
   "source": [
    "# use val to find the appropriate degree\n",
    "mpr = dict()\n",
    "for i in range(2,6):\n",
    "    degree=i\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    # polynomial transform\n",
    "    X_train_poly = poly.fit_transform(X_train_pca, y_train)\n",
    "    X_val_poly = poly.fit_transform(X_val_pca,y_val)\n",
    "    X_test_poly = poly.fit_transform(X_test_pca,y_test)\n",
    "\n",
    "    # fit to linear regression\n",
    "    polyreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "\n",
    "    # predict using liear regression\n",
    "    score_poly_reg = polyreg.score(X_val_poly, y_val)\n",
    "    print('score_poly_reg, degree',i,':', \"%.4f\"%score_poly_reg)\n",
    "    mpr[f'{i}'] = score_poly_reg\n",
    "    \n",
    "max_degree = max(mpr, key = mpr.get)\n",
    "print('max degree:', max_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05910034207855541"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(int(max_degree))\n",
    "# polynomial transform\n",
    "X_train_poly = poly.fit_transform(X_train_pca, y_train)\n",
    "X_val_poly = poly.fit_transform(X_val_pca,y_val)\n",
    "X_test_poly = poly.fit_transform(X_test_pca,y_test)\n",
    "\n",
    "# fit to linear regression\n",
    "polyreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "\n",
    "# predict using liear regression\n",
    "score_poly_reg = polyreg.score(X_test_poly, y_test)\n",
    "score_poly_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max depth: 7\n"
     ]
    }
   ],
   "source": [
    "# testing for different max depth\n",
    "rfc = dict()\n",
    "for i in range (1, 15):\n",
    "    clf = RandomForestClassifier(max_depth=i, random_state=0).fit(X_train_c_pca, y_train_c)\n",
    "    y_pred = clf.predict(X_val_c_pca)\n",
    "    score_clf = accuracy_score(y_val_c, y_pred)\n",
    "    rfc[f'{i}'] = score_clf\n",
    "    #print('score_clf_',i,\":\", \"%.4f\"%score_clf)\n",
    "max_depth = max(rfc, key=rfc.get)\n",
    "print('max depth:', max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_rf_clf: 0.6353\n"
     ]
    }
   ],
   "source": [
    "# choose max_depth = 4\n",
    "clf = RandomForestClassifier(max_depth=int(max_depth), random_state=0).fit(X_train_c_pca, y_train_c)\n",
    "score_rf_clf = clf.score(X_test_c_pca, y_test_c)\n",
    "print('score_rf_clf:', \"%.4f\"%score_rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_rf_reg: 0.0126\n"
     ]
    }
   ],
   "source": [
    "clf_reg = RandomForestRegressor().fit(X_train_pca, y_train)\n",
    "score_rf_reg = clf_reg.score(X_test_pca, y_test)\n",
    "print('score_rf_reg:', \"%.4f\"%score_rf_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_logistic: 0.7176\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(penalty = 'l1', solver='liblinear', random_state=0).fit(X_train_c_pca, y_train_c)\n",
    "score_logistic = logistic_model.score(X_test_c_pca, y_test_c)\n",
    "print('score_logistic:', \"%.4f\"%score_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5764705882352941"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gra_clf = GradientBoostingClassifier(n_estimators = 200, learning_rate=0.50, max_depth=5, random_state=0).fit(X_train_c_pca, y_train_c)\n",
    "score_gb_clf = gra_clf.score(X_test_c_pca, y_test_c)\n",
    "score_gb_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15659311913130036"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gra_reg = GradientBoostingRegressor(random_state=0).fit(X_train_pca, y_train)\n",
    "score_gb_reg = gra_reg.score(X_test_pca, y_test)\n",
    "score_gb_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:15:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lemon\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5764705882352941"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier().fit(X_train_c_pca, y_train_c)\n",
    "score_xgb_clf = xgb_clf.score(X_test_c_pca, y_test_c)\n",
    "score_xgb_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2369724167947389"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = XGBRegressor(random_state = 0).fit(X_train_pca, y_train)\n",
    "score_xgb_reg = xgb_reg.score(X_test_pca, y_test)\n",
    "score_xgb_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611764705882353"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adb_clf = AdaBoostClassifier().fit(X_train_c_pca, y_train_c)\n",
    "score_adb_clf = adb_clf.score(X_test_c_pca, y_test_c)\n",
    "score_adb_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0528609994506003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adb_reg = AdaBoostRegressor().fit(X_train_pca, y_train)\n",
    "score_adb_reg = adb_reg.score(X_test_pca, y_test)\n",
    "score_adb_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6941176470588235\n",
      "0.6941176470588235\n",
      "0.7176470588235294\n"
     ]
    }
   ],
   "source": [
    "# Try different kernels\n",
    "kernels = ('linear', 'poly', 'rbf')\n",
    "\n",
    "for k in kernels:\n",
    "    svm_clf = SVC(kernel=k).fit(X_train_c_pca, y_train_c)\n",
    "    y_pred = svm_clf.predict(X_test_c_pca)\n",
    "    vars()[f'score_svc_{k}']= accuracy_score(y_test_c, y_pred)\n",
    "    \n",
    "print(score_svc_linear)\n",
    "print(score_svc_poly)\n",
    "print(score_svc_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6823529411764706\n"
     ]
    }
   ],
   "source": [
    "gnb_clf = GaussianNB().fit(X_train_c_pca, y_train_c)\n",
    "score_gnb = gnb_clf.score(X_test_c_pca, y_test_c)\n",
    "print(score_gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 2s 90ms/step - loss: 0.7041 - accuracy: 0.5076 - val_loss: 0.6961 - val_accuracy: 0.4706\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7071 - accuracy: 0.4965 - val_loss: 0.6941 - val_accuracy: 0.4706\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7026 - accuracy: 0.5098 - val_loss: 0.6927 - val_accuracy: 0.5059\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6951 - accuracy: 0.4820 - val_loss: 0.6918 - val_accuracy: 0.4941\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6878 - accuracy: 0.4916 - val_loss: 0.6906 - val_accuracy: 0.5059\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.4840 - val_loss: 0.6899 - val_accuracy: 0.5294\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5262 - val_loss: 0.6886 - val_accuracy: 0.5059\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5205 - val_loss: 0.6879 - val_accuracy: 0.5294\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5497 - val_loss: 0.6870 - val_accuracy: 0.5176\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6798 - accuracy: 0.5746 - val_loss: 0.6863 - val_accuracy: 0.5059\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6804 - accuracy: 0.5509 - val_loss: 0.6861 - val_accuracy: 0.5176\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6759 - accuracy: 0.5674 - val_loss: 0.6857 - val_accuracy: 0.5294\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.5867 - val_loss: 0.6860 - val_accuracy: 0.5176\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6728 - accuracy: 0.5977 - val_loss: 0.6853 - val_accuracy: 0.4941\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6701 - accuracy: 0.5620 - val_loss: 0.6843 - val_accuracy: 0.5059\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5619 - val_loss: 0.6839 - val_accuracy: 0.4941\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.6036 - val_loss: 0.6830 - val_accuracy: 0.5176\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.5882 - val_loss: 0.6816 - val_accuracy: 0.5294\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.5996 - val_loss: 0.6797 - val_accuracy: 0.5176\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6640 - accuracy: 0.6424 - val_loss: 0.6795 - val_accuracy: 0.5294\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6448 - val_loss: 0.6784 - val_accuracy: 0.5412\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.6278 - val_loss: 0.6776 - val_accuracy: 0.5059\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.6186 - val_loss: 0.6765 - val_accuracy: 0.5412\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6625 - accuracy: 0.6360 - val_loss: 0.6751 - val_accuracy: 0.5647\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6601 - accuracy: 0.6465 - val_loss: 0.6743 - val_accuracy: 0.5529\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6579 - accuracy: 0.6370 - val_loss: 0.6734 - val_accuracy: 0.5647\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6468 - accuracy: 0.6633 - val_loss: 0.6729 - val_accuracy: 0.5647\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6590 - accuracy: 0.6006 - val_loss: 0.6724 - val_accuracy: 0.5647\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6529 - accuracy: 0.6481 - val_loss: 0.6715 - val_accuracy: 0.5647\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6702 - accuracy: 0.6034 - val_loss: 0.6712 - val_accuracy: 0.5529\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6564 - accuracy: 0.6202 - val_loss: 0.6717 - val_accuracy: 0.5647\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6575 - accuracy: 0.6430 - val_loss: 0.6702 - val_accuracy: 0.5647\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6497 - accuracy: 0.6555 - val_loss: 0.6680 - val_accuracy: 0.5529\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.6260 - val_loss: 0.6661 - val_accuracy: 0.5529\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6438 - accuracy: 0.6414 - val_loss: 0.6662 - val_accuracy: 0.5412\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.6250 - val_loss: 0.6648 - val_accuracy: 0.5294\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6534 - accuracy: 0.6373 - val_loss: 0.6633 - val_accuracy: 0.5529\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6534 - accuracy: 0.6519 - val_loss: 0.6624 - val_accuracy: 0.5529\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6582 - accuracy: 0.6255 - val_loss: 0.6607 - val_accuracy: 0.5647\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.6513 - val_loss: 0.6587 - val_accuracy: 0.5765\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6511 - accuracy: 0.6301 - val_loss: 0.6576 - val_accuracy: 0.5765\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6395 - accuracy: 0.6652 - val_loss: 0.6568 - val_accuracy: 0.5765\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.6673 - val_loss: 0.6555 - val_accuracy: 0.5765\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6301 - accuracy: 0.6770 - val_loss: 0.6552 - val_accuracy: 0.5765\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6427 - accuracy: 0.6526 - val_loss: 0.6548 - val_accuracy: 0.6000\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6469 - accuracy: 0.6629 - val_loss: 0.6545 - val_accuracy: 0.6000\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6439 - accuracy: 0.6444 - val_loss: 0.6530 - val_accuracy: 0.6000\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6362 - accuracy: 0.6583 - val_loss: 0.6520 - val_accuracy: 0.6000\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6338 - accuracy: 0.6746 - val_loss: 0.6513 - val_accuracy: 0.6000\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6397 - accuracy: 0.6563 - val_loss: 0.6502 - val_accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(10, input_dim=num_var, activation='relu'))\n",
    "model.add(keras.layers.Dense(5, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_c_pca, y_train_c, epochs=50, validation_data=(X_val_c_pca, y_val_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6703 - accuracy: 0.6588\n",
      "0.658823549747467\n"
     ]
    }
   ],
   "source": [
    "loss, score_neuro = model.evaluate(X_test_c_pca, y_test_c)\n",
    "print(score_neuro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regression models</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multiple linear regression</th>\n",
       "      <td>0.073323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multiple polynomial regression</th>\n",
       "      <td>-0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest regressor</th>\n",
       "      <td>0.012619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient boosting regressor</th>\n",
       "      <td>-0.156593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost regressor</th>\n",
       "      <td>-0.236972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost regressor</th>\n",
       "      <td>0.052861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Score\n",
       "Regression models                       \n",
       "Multiple linear regression      0.073323\n",
       "Multiple polynomial regression -0.059100\n",
       "Random forest regressor         0.012619\n",
       "Gradient boosting regressor    -0.156593\n",
       "XGBoost regressor              -0.236972\n",
       "AdaBoost regressor              0.052861"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = {\n",
    "    'Multiple linear regression': score_multiple_reg,\n",
    "    'Multiple polynomial regression': score_poly_reg,\n",
    "    'Random forest regressor': score_rf_reg,\n",
    "    'Gradient boosting regressor': score_gb_reg,\n",
    "    'XGBoost regressor': score_xgb_reg,\n",
    "    'AdaBoost regressor': score_adb_reg\n",
    "}\n",
    "reg_df = pd.DataFrame.from_dict(reg, orient='index', columns = ['Score'])\n",
    "reg_df.index.name = 'Regression models'\n",
    "#reg_df.sort_values(by=['Score'], ascending=False)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier models</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>0.717647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest classifier</th>\n",
       "      <td>0.635294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient boosting classifier</th>\n",
       "      <td>0.576471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost classifier</th>\n",
       "      <td>0.576471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost classifier</th>\n",
       "      <td>0.611765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian naive Bayes</th>\n",
       "      <td>0.682353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC linear</th>\n",
       "      <td>0.694118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC polynomial</th>\n",
       "      <td>0.694118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC RBF</th>\n",
       "      <td>0.717647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neuro network</th>\n",
       "      <td>0.658824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Score\n",
       "Classifier models                     \n",
       "Logistic regression           0.717647\n",
       "Random forest classifier      0.635294\n",
       "Gradient boosting classifier  0.576471\n",
       "XGBoost classifier            0.576471\n",
       "AdaBoost classifier           0.611765\n",
       "Gaussian naive Bayes          0.682353\n",
       "SVC linear                    0.694118\n",
       "SVC polynomial                0.694118\n",
       "SVC RBF                       0.717647\n",
       "Neuro network                 0.658824"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = {\n",
    "    'Logistic regression': score_logistic,\n",
    "    'Random forest classifier': score_rf_clf,\n",
    "    'Gradient boosting classifier': score_gb_clf,\n",
    "    'XGBoost classifier': score_xgb_clf,\n",
    "    'AdaBoost classifier': score_adb_clf,\n",
    "    'Gaussian naive Bayes': score_gnb, \n",
    "    'SVC linear': score_svc_linear,\n",
    "    'SVC polynomial': score_svc_poly, \n",
    "    'SVC RBF': score_svc_rbf,\n",
    "    'Neuro network' : score_neuro\n",
    "}\n",
    "clf_df = pd.DataFrame.from_dict(clf, orient='index',columns = ['Score'])\n",
    "clf_df.index.name = 'Classifier models'\n",
    "#clf_df.sort_values(by=['Score'], ascending=False)\n",
    "clf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
